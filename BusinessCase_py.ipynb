{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP9CePHK3XXa64jaOCngZF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rish0x29/Python/blob/main/BusinessCase_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1pqLda_T0VJL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# We will use the sklearn preprocessing library, as it will be easier to standardize the data.\n",
        "from sklearn import preprocessing\n",
        "\n",
        "raw_csv_data = np.loadtxt('/content/sample_data/Audiobooks_data.csv',delimiter=',')\n",
        "\n",
        "# The inputs are all columns in the csv, except for the first one [:,0]\n",
        "# and the last one [:,-1] (which is our targets)\n",
        "\n",
        "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
        "targets_all = raw_csv_data[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_indices = np.arange(unscaled_inputs_all.shape[0])\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "unscaled_inputs_all = unscaled_inputs_all[shuffled_indices]\n",
        "targets_all = targets_all[shuffled_indices]"
      ],
      "metadata": {
        "id": "z_n60cpw94yK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_one_targets = int(np.sum(targets_all))\n",
        "\n",
        "zero_targets_counter = 0\n",
        "\n",
        "# We want to create a \"balanced\" dataset, so we will have to remove some input/target pairs.\n",
        "indices_to_remove = []\n",
        "\n",
        "# Count the number of targets that are 0.\n",
        "for i in range(targets_all.shape[0]):\n",
        "    if targets_all[i] == 0:\n",
        "        zero_targets_counter += 1\n",
        "        if zero_targets_counter > num_one_targets:\n",
        "            indices_to_remove.append(i)\n",
        "\n",
        "# Create two new variables, one that will contain the inputs, and one that will contain the targets.\n",
        "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
        "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
      ],
      "metadata": {
        "id": "1Doc70YJ-CNW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It's a simple line of code, which standardizes the inputs, as we explained in one of the lectures.\n",
        "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
      ],
      "metadata": {
        "id": "KErIsERB-EqE"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.\n",
        "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
        "shuffled_targets = targets_equal_priors[shuffled_indices]"
      ],
      "metadata": {
        "id": "p6NfG58x-GcI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples_count = shuffled_inputs.shape[0]\n",
        "\n",
        "train_samples_count = int(0.8 * samples_count)\n",
        "validation_samples_count = int(0.1 * samples_count)\n",
        "\n",
        "# The 'test' dataset contains all remaining data.\n",
        "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
        "\n",
        "train_inputs = shuffled_inputs[:train_samples_count]\n",
        "train_targets = shuffled_targets[:train_samples_count]\n",
        "\n",
        "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
        "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
        "\n",
        "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
        "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
        "\n",
        "\n",
        "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
        "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
        "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
        "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h4ryhW2-JQW",
        "outputId": "f9e649bf-3b27-4d6b-a147-f16e96bcfd5f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1779.0 3579 0.49706621961441744\n",
            "219.0 447 0.4899328859060403\n",
            "239.0 448 0.5334821428571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
        "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
        "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
      ],
      "metadata": {
        "id": "kQJ0V3rH-LO6"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "sOnb5k7QI4_r"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npz = np.load('Audiobooks_data_train.npz')\n",
        "\n",
        "train_inputs = npz['inputs'].astype(float)\n",
        "train_targets = npz['targets'].astype(int)\n",
        "\n",
        "# we load the validation data in the temporary variable\n",
        "npz = np.load('Audiobooks_data_validation.npz')\n",
        "# we can load the inputs and the targets in the same line\n",
        "validation_inputs, validation_targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
        "\n",
        "# we load the test data in the temporary variable\n",
        "npz = np.load('Audiobooks_data_test.npz')\n",
        "# we create 2 variables that will contain the test inputs and the test targets\n",
        "test_inputs, test_targets = npz['inputs'].astype(float), npz['targets'].astype(int)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hZXpoJw3I63L"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 10\n",
        "output_size = 2\n",
        "hidden_layer_size = 50\n",
        "\n",
        "# define how the model will look like\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
        "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# the loss function,\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "### Training\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "max_epochs = 100\n",
        "\n",
        "# set an early stopping mechanism\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "\n",
        "model.fit(train_inputs,\n",
        "          train_targets,\n",
        "          batch_size=batch_size,\n",
        "          epochs=max_epochs,\n",
        "          callbacks=[early_stopping],\n",
        "          validation_data=(validation_inputs, validation_targets),\n",
        "          verbose = 2\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcR5WmMfJCJ2",
        "outputId": "6b78263b-e94e-4c72-d8e3-f8ddc7737e1d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 - 1s - 33ms/step - accuracy: 0.6980 - loss: 0.5830 - val_accuracy: 0.7696 - val_loss: 0.4757\n",
            "Epoch 2/100\n",
            "36/36 - 0s - 4ms/step - accuracy: 0.7619 - loss: 0.4621 - val_accuracy: 0.7919 - val_loss: 0.4109\n",
            "Epoch 3/100\n",
            "36/36 - 0s - 5ms/step - accuracy: 0.7888 - loss: 0.4164 - val_accuracy: 0.7919 - val_loss: 0.3879\n",
            "Epoch 4/100\n",
            "36/36 - 0s - 5ms/step - accuracy: 0.7916 - loss: 0.3951 - val_accuracy: 0.8121 - val_loss: 0.3747\n",
            "Epoch 5/100\n",
            "36/36 - 0s - 6ms/step - accuracy: 0.7999 - loss: 0.3810 - val_accuracy: 0.8009 - val_loss: 0.3683\n",
            "Epoch 6/100\n",
            "36/36 - 0s - 3ms/step - accuracy: 0.8078 - loss: 0.3724 - val_accuracy: 0.8076 - val_loss: 0.3594\n",
            "Epoch 7/100\n",
            "36/36 - 0s - 3ms/step - accuracy: 0.8066 - loss: 0.3664 - val_accuracy: 0.8188 - val_loss: 0.3561\n",
            "Epoch 8/100\n",
            "36/36 - 0s - 3ms/step - accuracy: 0.8064 - loss: 0.3642 - val_accuracy: 0.8031 - val_loss: 0.3557\n",
            "Epoch 9/100\n",
            "36/36 - 0s - 3ms/step - accuracy: 0.8117 - loss: 0.3596 - val_accuracy: 0.7964 - val_loss: 0.3535\n",
            "Epoch 10/100\n",
            "36/36 - 0s - 3ms/step - accuracy: 0.8080 - loss: 0.3592 - val_accuracy: 0.8076 - val_loss: 0.3500\n",
            "Epoch 11/100\n",
            "36/36 - 0s - 3ms/step - accuracy: 0.8136 - loss: 0.3550 - val_accuracy: 0.8031 - val_loss: 0.3484\n",
            "Epoch 12/100\n",
            "36/36 - 0s - 3ms/step - accuracy: 0.8106 - loss: 0.3530 - val_accuracy: 0.8322 - val_loss: 0.3507\n",
            "Epoch 13/100\n",
            "36/36 - 0s - 3ms/step - accuracy: 0.8100 - loss: 0.3576 - val_accuracy: 0.7964 - val_loss: 0.3578\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d514488e7e0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd0y0P6IJGBy",
        "outputId": "2aa073db-4c22-4dc7-8db8-7a529526d762"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7964 - loss: 0.3636 \n"
          ]
        }
      ]
    }
  ]
}