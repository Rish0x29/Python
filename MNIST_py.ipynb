{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVdVftEUF+oEv6XT732/tU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rish0x29/Python/blob/main/MNIST_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_zJipx1bBdR",
        "outputId": "ddd3ddea-8dbc-4c0a-dca6-0e9eca96951c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modules Imported\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "print('Modules Imported')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
        "\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
        "\n",
        "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64) # let's cast this number to an integer, as a float may cause an error along the way\n",
        "\n",
        "\n",
        "# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "\n",
        "def scale(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255.         # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1\n",
        "\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "scaled_train_and_validation_data = mnist_train.map(scale) # .map() allows us to apply a custom transformation to a given dataset\n",
        "\n",
        "\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "BUFFER_SIZE = 10000 # this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets\n",
        "\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)  #similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset\n",
        "\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))"
      ],
      "metadata": {
        "id": "zdHVIAPAcxiO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "output_size = 10\n",
        "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
        "hidden_layer_size = 50\n",
        "\n",
        "# define how the model will look like\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "    # the first layer (the input layer)\n",
        "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
        "    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n",
        "    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,)\n",
        "    # or (28x28x1,) = (784,) vector\n",
        "    # this allows us to actually create a feed forward neural network\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
        "\n",
        "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
        "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
        "\n",
        "    # the final layer is no different, we just make sure to activate it with softmax\n",
        "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
        "])\n",
        "# we define the optimizer we'd like to use,\n",
        "# the loss function,\n",
        "# and the metrics we are interested in obtaining at each iteration\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# determine the maximum number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# we fit the model, specifying the\n",
        "# training data\n",
        "# the total number of epochs\n",
        "# and the validation data we just created ourselves in the format: (inputs,targets)\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"
      ],
      "metadata": {
        "id": "arHIM_Urdlya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e4f753-954e-4e22-f232-82dc4e48667d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "540/540 - 12s - 21ms/step - accuracy: 0.8868 - loss: 0.4021 - val_accuracy: 0.9362 - val_loss: 0.2232\n",
            "Epoch 2/5\n",
            "540/540 - 6s - 11ms/step - accuracy: 0.9466 - loss: 0.1847 - val_accuracy: 0.9500 - val_loss: 0.1686\n",
            "Epoch 3/5\n",
            "540/540 - 9s - 16ms/step - accuracy: 0.9577 - loss: 0.1445 - val_accuracy: 0.9570 - val_loss: 0.1430\n",
            "Epoch 4/5\n",
            "540/540 - 5s - 10ms/step - accuracy: 0.9647 - loss: 0.1194 - val_accuracy: 0.9657 - val_loss: 0.1141\n",
            "Epoch 5/5\n",
            "540/540 - 4s - 7ms/step - accuracy: 0.9701 - loss: 0.1008 - val_accuracy: 0.9668 - val_loss: 0.1118\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f622e10a540>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}